training_config:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  num_train_epochs: 1
  dataloader_num_workers: 1
  fp16: true
  optim: "adamw_torch"
  learning_rate: 10.0e-5
  logging_steps: 1
  evaluation_strategy: "steps"
  save_strategy: "steps"
  eval_steps: 40
  save_steps: 40
  save_total_limit: 1
  deepspeed: ./configs/deepspeed/ds_config_zero1.json
  output_dir: ./output/
  report_to: "wandb"


model_config:
  fp16: false
  pretrained_path: # None or path to model weight
  model_type: git_llm
  language_model_name: TheBloke/Llama-2-7b-Chat-GPTQ
  vision_model_name: openai/clip-vit-base-patch16
  num_image_with_embedding: 1 # if 1, no img_temporal_embedding
  max_length: 512
  keys_to_finetune:
    - visual_projection
    - num_image_with_embedding
  keys_to_freeze: []

  use_lora: true
  lora:
    r: 1
    lora_alpha: 1
    target_modules:
      - q_proj
      - k_proj
      - v_proj
    lora_dropout: 0.001
    bias: none
    task_type: CAUSAL_LM

dataset_config_path:
  - /content/heron/configs/datasets/my_csv_config.yaml
